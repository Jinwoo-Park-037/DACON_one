{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01bd73dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/workspace/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import accelerate\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f336106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npkill -u \"$USER\" -f python\\nif memory is insufficient\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pkill -u \"$USER\" -f python\n",
    "if memory is insufficient\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b40c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./models\"          # download/cache here\n",
    "INPUT_CSV = \"test.csv\"          # expects columns: ID, Question\n",
    "OUTPUT_CSV = \"beomi_polyglot_5.csv\"   # submission format: ID, Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b646bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards:  18%|█▊        | 5/28 [00:02<00:11,  2.00it/s]"
     ]
    }
   ],
   "source": [
    "# model_id = \"snunlp/KR-Medium\"  # GPT2-like Korean LM\n",
    "# model_id = \"EleutherAI/polyglot-ko-5.8b\"  # Mistral-based KULLM successor (wasn't very good)\n",
    "model_id = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./models\", padding_side=\"left\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,                 # or load_in_8bit=True\n",
    "    device_map=\"auto\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "    return f\"\"\"질문에 답하세요.\n",
    "    - 우선 질문이 객관식인지 주관식인지 판단하세요\n",
    "    - 객관식이면 숫자 하나를 답으로 쓰세요 (하나만 정답입니다).\n",
    "    - 주관식이면 한 단락으로 질문의 모든 요소에 답하세요\n",
    "\n",
    "    질문:\n",
    "    {question}\n",
    "\n",
    "    답:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "test_df = pd.read_csv(INPUT_CSV)\n",
    "assert {\"ID\", \"Question\"}.issubset(test_df.columns), \"test.csv must have columns: ID, Question\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNI_DIGIT_MAP = str.maketrans({\n",
    "    \"①\":\"1\",\"②\":\"2\",\"③\":\"3\",\"④\":\"4\",\"⑤\":\"5\",\n",
    "    \"⑴\":\"1\",\"⑵\":\"2\",\"⑶\":\"3\",\"⑷\":\"4\",\"⑸\":\"5\",\n",
    "    \"❶\":\"1\",\"❷\":\"2\",\"❸\":\"3\",\"❹\":\"4\",\"❺\":\"5\",\n",
    "})\n",
    "\n",
    "def extract_answer_from_output(output_text: str) -> str:\n",
    "    # after \"답:\"\n",
    "    if \"답:\" in output_text:\n",
    "        ans = output_text.split(\"답:\", 1)[1].strip()\n",
    "    else:\n",
    "        ans = output_text.strip()\n",
    "\n",
    "    # normalize circled digits\n",
    "    ans = ans.translate(UNI_DIGIT_MAP).strip()\n",
    "\n",
    "    # if starts with number 1-5 → return only that\n",
    "    if ans and ans[0] in \"12345\":\n",
    "        return ans[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [16:31<00:00, 15.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 8  # tune: 4~16 depending on VRAM\n",
    "prompts = [build_prompt(str(q)) for q in test_df[\"Question\"]]\n",
    "\n",
    "answers = []\n",
    "model.eval()\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        batch_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,           # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "    enc = {k: v.to(model.device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=200,         \n",
    "            do_sample=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.05,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,             # (default True) keep it on\n",
    "            num_beams=1                 # beam search is slower\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "    answers.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [extract_answer_from_output(text) for text in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"Answer\": answers\n",
    "})\n",
    "\n",
    "submission.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd00fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
