{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01bd73dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/workspace/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import accelerate\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f336106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npkill -u \"$USER\" -f python\\nif memory is insufficient\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pkill -u \"$USER\" -f python\n",
    "if memory is insufficient\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b40c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./models\"          # download/cache here\n",
    "INPUT_CSV = \"test.csv\"          # expects columns: ID, Question\n",
    "OUTPUT_CSV = \"beomi_polyglot_2.csv\"   # submission format: ID, Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b646bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 28/28 [00:14<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"snunlp/KR-Medium\"  # GPT2-like Korean LM\n",
    "# model_id = \"EleutherAI/polyglot-ko-5.8b\"  # Mistral-based KULLM successor (wasn't very good)\n",
    "model_id = \"beomi/KoAlpaca-Polyglot-12.8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"./models\", padding_side=\"left\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,                 # or load_in_8bit=True\n",
    "    device_map=\"auto\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362c1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "    return f\"\"\"질문에 답하세요.\n",
    "    - 객관식이면 1~5 중 숫자 하나만 쓰고, 설명을 덧붙이지 마세요.\n",
    "    - 주관식이면 한 단락으로 간결하게 질문의 모든 요소에 답하세요\n",
    "    - 프롬프트를 반복하지 마세요.\n",
    "\n",
    "    질문:\n",
    "    {question}\n",
    "\n",
    "    답:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f856c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "test_df = pd.read_csv(INPUT_CSV)\n",
    "assert {\"ID\", \"Question\"}.issubset(test_df.columns), \"test.csv must have columns: ID, Question\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827327fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude circled numbers: e.g. ①\n",
    "circled = [chr(c) for c in range(0x2460, 0x2474)] \n",
    "\n",
    "bad_ids = []\n",
    "\n",
    "for ch in circled:\n",
    "    ids = tokenizer.encode(ch, add_special_tokens=False)\n",
    "    if len(ids) == 1:            # only ban if it’s a single token\n",
    "        bad_ids.append([ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c06c4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_output(output_text):\n",
    "    \"\"\"\n",
    "    LLM이 생성한 출력에서 '답:' 이후 텍스트만 추출\n",
    "    \"\"\"\n",
    "    if \"답:\" in output_text:\n",
    "        answer_part = output_text.split(\"답:\")[-1].strip()\n",
    "    else:\n",
    "        answer_part = output_text.strip() \n",
    "    \n",
    "    match = re.match(r\"^\\s*([1-5])\\b\", answer_part)\n",
    "    if match:\n",
    "        return match.group(1)  # 첫 번째 숫자만 반환\n",
    "\n",
    "    return answer_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 36/65 [09:12<07:24, 15.33s/it]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 8  # tune: 4~16 depending on VRAM\n",
    "prompts = [build_prompt(str(q)) for q in test_df[\"Question\"]]\n",
    "\n",
    "answers = []\n",
    "model.eval()\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        batch_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,           # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "    enc = {k: v.to(model.device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=80,          # ↓ see tip #2\n",
    "            do_sample=False,            # greedy is fastest\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.05,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,             # (default True) keep it on\n",
    "            num_beams=1                 # beam search is slower\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "    answers.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [extract_answer_from_output(text) for text in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38edde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"Answer\": answers\n",
    "})\n",
    "\n",
    "submission.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd00fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
